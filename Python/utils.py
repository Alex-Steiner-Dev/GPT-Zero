# Tokenize the sentence
def tokenize_sentence(sentence):
    return sentence.split()